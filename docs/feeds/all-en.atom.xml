<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Brian Mc Donald</title><link href="https://brianmcdonald.me/" rel="alternate"></link><link href="https://brianmcdonald.me/feeds/all-en.atom.xml" rel="self"></link><id>https://brianmcdonald.me/</id><updated>2023-07-22T18:35:00+02:00</updated><subtitle>Humanitarian Information Management Specialist</subtitle><entry><title>Introducing XLSform filler data</title><link href="https://brianmcdonald.me/XLSform-filler-data.html" rel="alternate"></link><published>2023-07-22T18:35:00+02:00</published><updated>2023-07-22T18:35:00+02:00</updated><author><name>Brian</name></author><id>tag:brianmcdonald.me,2023-07-22:/XLSform-filler-data.html</id><summary type="html">&lt;p&gt;A tool for creating sample data for testing and drafting&amp;nbsp;analysis-plans.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;Analysis tools such as Python and R provide many benefits for humaniatarian analysis. They provide reproducilble analysis flows which are great for collaborative analysis, provide an approach than can build reuse and build upon prior analysis and importantly, provide an auditable, robust and reproducible&amp;nbsp;approach. &lt;/p&gt;
&lt;p&gt;Another of their strengths is how it enables faster turn-around on analyitical prouducts due to its reuse of code and methods. This is the case in situations where the analytical product design remains very close to previous examples, but what about the many cases cases where the output analysis needs to be adjusted to fit the context or a new methodology or mix of methods. In these cases one of the most limiting factors in the analysis turnaround time is the period where the analyst needs to wait for data inputs or produce example data in order to start develoing their data analysis plan or draft their&amp;nbsp;analysis. &lt;/p&gt;
&lt;p&gt;A tool that can simulate data can radically reduce the time between data collection and the production date of its analytical&amp;nbsp;outputs. &lt;/p&gt;
&lt;h2&gt;A&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;That&amp;#8217;s what &lt;em&gt;XLSform filler data&lt;/em&gt; aim to accomplish. Using as an input, an XLSform survey, the tool creates a sample survey dataset which can be used in generating a analysis plan and drafting of the initial analysis, allowing the analyst to prepare the descriptive analysis sections in advance of the data collection period, freeing the analyst up to focus on the higher levels of the analysis spectrum, the explanations, the interpretations, the anticipations and the&amp;nbsp;prescriptions. &lt;/p&gt;
&lt;p&gt;The tool is written in and is focused on the Python ecosystem. its published on PyPi and can be installed with the following command &lt;code&gt;pip install XLSform-data-filler&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;To create a dummy dataset, with a default number of rows(100) from a XLSform source: &lt;code&gt;xlsform-filler-data &amp;lt;source-file-path&amp;gt;/&amp;lt;filename.xlsx&amp;gt;&lt;/code&gt;
To specify the number of rows to create, use the -r flag. Example: &lt;code&gt;xlsform-filler-data &amp;lt;source-file-path&amp;gt;/&amp;lt;filename.xlsx&amp;gt; -r 200&lt;/code&gt;
To specify the output path and filename, pass the -o flag. Example: &lt;code&gt;xlsform-filler-data &amp;lt;source-file-path&amp;gt;/&amp;lt;filename.xlsx&amp;gt; -o &amp;lt;./myfile.xlsx&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;As of version 0.1.1 the tool does not properly randomise multiple choice questions; omits some variables such as &amp;#8216;start&amp;#8217; and &amp;#8216;end&amp;#8217;; does not maintain the order of the variables; and does not incorporate constraints or cascading choice lists. These limitations will be adddressed in future&amp;nbsp;releases.&lt;/p&gt;</content><category term="writing"></category><category term="analysis"></category><category term="python"></category><category term="xlsform"></category><category term="kobo"></category></entry><entry><title>Towards Unified Data Layers</title><link href="https://brianmcdonald.me/unified-data-layers.html" rel="alternate"></link><published>2022-10-27T19:40:00+02:00</published><updated>2023-04-16T14:04:00+02:00</updated><author><name>Brian Mc Donald</name></author><id>tag:brianmcdonald.me,2022-10-27:/unified-data-layers.html</id><summary type="html">&lt;p&gt;Common Operating Datasets in humanitarian analysis are amazing&amp;#8230;. so lets replace&amp;nbsp;them&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="painting" src="../files/images/501450ldsdl.jpg"&gt;
&lt;em&gt;A Capriccio of Roman Ruins (1727-1729) by Marco&amp;nbsp;Ricci&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The opinions below, as always, are mine alone, and do not represent my employer&amp;#8217;s, past or&amp;nbsp;present.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Common Operating&amp;nbsp;Datasets&lt;/h2&gt;
&lt;p&gt;The work done on Common Operating Datasets (CODs) has been one of the most important advances in information management in humanitarian response in the past 20&amp;nbsp;years.&lt;/p&gt;
&lt;p&gt;Common Operating Datasets are typically country level administrative boundaries and population statistics. They are the basic building blocks of a response. In an emergency, with hundreds of organizations and government departments working together, they provide a common point of reference for geographical representation of areas and population  within a country. In countries where there is a &lt;span class="caps"&gt;UNOCHA&lt;/span&gt; presence, they usually take the lead in the publishing of that country&amp;#8217;s CODs, in coordination with the relevant government statistics office and relevant partners, revising and updating as needed. CODs are probably the most successful example of coordination of information management. [footnote link to Andrew Verity recent&amp;nbsp;publication]&lt;/p&gt;
&lt;p&gt;But&amp;#8230; the purpose of this post is not just to praise the work on CODs, it&amp;#8217;s to demonstrate emerging alternatives that overcome many of the issues we face with&amp;nbsp;CODs. &lt;/p&gt;
&lt;h3&gt;So what&amp;#8217;s wrong with the&amp;nbsp;CODs?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;The CODs are not always available at a level of granularity sufficient for detailed analysis.&lt;/strong&gt; While admin 0 (national-level), admin 1 (typically provincial-level or similar), and admin 2 (typically district-level or similar) can be expected to present in any response with a &lt;span class="caps"&gt;UNOCHA&lt;/span&gt; presence, there can be less of an assumption of availability for the more granular levels of admin 3, 4 or lower. This presents a challenge as analysis may then be presenting analysis at a coarser level of aggregation that may not effectively capture or show differences in needs, gaps or response that a more granular administrative and population would&amp;nbsp;support.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Admin boundaries are often disputed.&lt;/strong&gt; While many national boundaries are disputed, to varying degrees [ footnote of how many, from Wikipedia] this generally isn&amp;#8217;t a huge issue in most humanitarian responses. In comparison to, the challenge of different perspectives on sub-national boundaries within countries. This can lead to a situation where, depending on engagement at the national level, or with subnational authorities, different datasets may be needed, top reflect of the differing perspectives of what area or people that are being referred to. For compiling data, this means the added complexity of needing to know the context of the data received, to understand the perspectives of the data and the assumptions behind&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;High variation in the area and population that a &lt;span class="caps"&gt;COD&lt;/span&gt; unit is referring to.&lt;/strong&gt; As mentioned above, the number of admin levels in a country can vary, what varies even greater, is the geographic area that &lt;span class="caps"&gt;COD&lt;/span&gt; units represent across countries. An analysis, that may be sufficiently granular in one country, may need to be at admin 3 or 4 in another to provide a similar analysis. While a typical analysis product may have a geographical area that may be optimal for the type of analysis, quite often this has to first match the closest available admin boundaries and population, assuming all required data is also available for&amp;nbsp;this. &lt;/p&gt;
&lt;p&gt;Within a single country, this variation in admin unit size can lead to misinterpretation of data when visualizing data. A common example of this is with the use of chloropleth maps. Considering two aspects of visual perception - colour and size, chloropleth&amp;#8217;s often mislead, showing comparatively larger admin areas more prominently, even though their value (colour) may be the same or less. This misuse of choropleth graphics probably deserves an article by itself. [footnote showing the km2 area of the largest admin 3 and smallest admin2 from the&amp;nbsp;CODs]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CODs are constantly changing&lt;/strong&gt;, they are living datasets that periodically need updating to reflect administrative changes and population changes in a country. These administrative changes can mean that data from previous years may no longer be compatible with that which was gathered in different years [footnote Pakistan example of admin 1 changes] Currently the &lt;span class="caps"&gt;COD&lt;/span&gt;&amp;#8217;s don&amp;#8217;t have a process for version control, meaning that the changes of each iteration may not be known, meaning that datasets associated with previous &lt;span class="caps"&gt;COD&lt;/span&gt; versions may no longer be compatible, or may require significant data transformations or may contain a number of hidden&amp;nbsp;assumptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Demographics and administrative boundaries are intertwined with politics.&lt;/strong&gt; Administrative boundaries are, as the name suggests, reflect the administrative structure of a country. They are a cartographic manifestation of much of our countries historic dynamics, current political dynamics, especially in conflict situations, can themselves be drivers, or at least contentious within the context. The close link of the geographic and the political in-turn influences any analysis that uses them. Even in non, or low-conflict contexts, this can still mean challenges, such as delays in publishing or updating datasets due to bureaucracy or otherwise. [ footnote Idai&amp;nbsp;example]  &lt;/p&gt;
&lt;h2&gt;Introducing H3 and Bing&amp;nbsp;quadkeys&lt;/h2&gt;
&lt;p&gt;One approach we can use that addresses or sidesteps the problems above to separate political &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; administrative with geographic concerns and base our geographic units of analysis on units that are consistent no matter what country, context or period the data is&amp;nbsp;from.  &lt;/p&gt;
&lt;p&gt;There are many ways other than &lt;span class="caps"&gt;COD&lt;/span&gt;&amp;#8217;s that can be used to separate and represent different geographic units, many reach back thousands of years [footnote mandala ] but in the past few years, with the growth in web-based mapping, large-scale analytics and the explosion of geo-associated data, some interesting approaches have emerged that are showing a lot of potential for humanitarian&amp;nbsp;action.&lt;/p&gt;
&lt;p&gt;&lt;img alt="quadkeys" src="../files/images/quadkeys.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The first of these are &lt;strong&gt;Quadkeys&lt;/strong&gt; an approach to geospatial indexing, developed by Microsoft, that divided the world into gridded squares [add footnote of Mercator projection limitation] with the size of each square corresponding to an index level. Each square can be subdivided further through subsequent lower levels, allowing one square to represent over 78,271 m2 at its highest level, or 0.0187 m2 at its lowest&amp;nbsp;level. &lt;/p&gt;
&lt;p&gt;The second is &lt;strong&gt;H3&lt;/strong&gt;, a similar concept, using hexagons. Developed by Uber, it provides a base for their vast geospatial analytic needs on car pickups, movements, routing etc. 
&lt;img alt="h3geo" src="../files/images/H3geo.PNG"&gt; &lt;/p&gt;
&lt;h2&gt;Their&amp;nbsp;advantages&lt;/h2&gt;
&lt;p&gt;These two approaches overcome many of the drawbacks of the CODs&amp;nbsp;as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the resolution/granularity is set to whatever unit that best suits the analysis, or which best matches the available&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;each quadkey/hex remains consistent and are inherently immune to disputes over their boundaries or&amp;nbsp;names.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for any given quadkey/hex value, their areas remain consistent and comparable [add footnote explaining limitations/approximations due to &lt;span class="caps"&gt;CRS&lt;/span&gt;], &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they don&amp;#8217;t change; a quadkey of data from 100 years can be compared against a quadkey of data 100 years from now, regardless of any changes in administrative boundaries or naming&amp;nbsp;conventions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they offer a means to sidestep the issue of politics intertwined with&amp;nbsp;geography.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they encourage privacy-preserving best practices for data&amp;nbsp;analysis. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The&amp;nbsp;possibilities&lt;/h2&gt;
&lt;p&gt;Imagine if, instead of struggling to reconcile development data that uses outdated admin boundaries into your humanitarian dataset, you can combine them seamlessly. Imagine, instead of using choropleths at wide admin-levels to show &lt;span class="caps"&gt;NGO&lt;/span&gt; partner presence, you present their presence as H3 cells at a resolution that better matches the catchment area of the services they are providing. Imagine being able to easily consolidate all your data, from demographics, displacement data, climate data, hazard risk data, household vulnerability data, to the same geographic unit of analysis to better inform prioritization and targeting of&amp;nbsp;interventions.&lt;/p&gt;
&lt;p&gt;If you are interested in finding out more about the unified data layer approach or would like to help me develop tools to accelerate its use, or if you have examples that you would like to share, please get in touch with&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The start of this post presents unified data layers as a replacement to &lt;span class="caps"&gt;COD&lt;/span&gt;&amp;#8217;s. This is done more for shock value, to stimulate conversation, than in a belief that they could or should replace &lt;span class="caps"&gt;COD&lt;/span&gt;&amp;#8217;s. Using admin-levels is preferable much of the time, as there is a clear need to understand and respond using geographic administrative structures that countries use to administer and&amp;nbsp;govern.&lt;/p&gt;</content><category term="General"></category><category term="geospatial"></category><category term="deckgl"></category><category term="h3"></category><category term="quadkeys"></category><category term="analysis"></category></entry><entry><title>Record matching in humanitarian data</title><link href="https://brianmcdonald.me/record-matching-in-humanitarian-data.html" rel="alternate"></link><published>2020-09-20T14:29:00+02:00</published><updated>2020-09-20T14:29:00+02:00</updated><author><name>Brian Mc Donald</name></author><id>tag:brianmcdonald.me,2020-09-20:/record-matching-in-humanitarian-data.html</id><summary type="html">&lt;p&gt;Using approaches such as &amp;#8216;probilistic record linkage&amp;#8217; to join humanitarian&amp;nbsp;datasets.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A reoccurring scenario in information management in humanitarian response is one where you have to compile messy data from a number of disparate sources within a very short time window. This post is an attempt to summarise some of the tools and approaches I use to tackle this&amp;nbsp;problem.&lt;/p&gt;
&lt;h2&gt;Excel: &lt;span class="caps"&gt;VLOOKUP&lt;/span&gt;, &lt;span class="caps"&gt;XLOOKUP&lt;/span&gt; and &lt;span class="caps"&gt;INDEXMATCH&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;While I wont cover them here, using &lt;a href="https://support.microsoft.com/en-us/office/vlookup-function-0bbc8083-26fe-4963-8ab8-93a18ad188a1"&gt;&lt;span class="caps"&gt;VLOOKUP&lt;/span&gt;&lt;/a&gt;, &lt;a href="https://support.microsoft.com/en-us/office/look-up-values-with-vlookup-index-or-match-68297403-7c3c-4150-9e3c-4d348188976b"&gt;&lt;span class="caps"&gt;INDEX&lt;/span&gt; &lt;span class="caps"&gt;MATCH&lt;/span&gt;&lt;/a&gt; and the newer &lt;a href="https://support.microsoft.com/en-us/office/xlookup-function-b7fd680e-6d10-43e6-84f9-88eae8bf5929"&gt;&lt;span class="caps"&gt;XLOOKUP&lt;/span&gt;&lt;/a&gt; in Excel are really useful formula when trying to combine different datasets in Excel. Learning these for the first time really feels like unlocking an analysis&amp;nbsp;superpower. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;=XLOOKUP (lookup, lookup_array, return_array)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When using &lt;span class="caps"&gt;VLOOKUP&lt;/span&gt; or &lt;span class="caps"&gt;XLOOKUP&lt;/span&gt; you can shoose to use &amp;#8216;&lt;span class="caps"&gt;FALSE&lt;/span&gt;&amp;#8221; at the end of the formula or &amp;#8216;&lt;span class="caps"&gt;TRUE&lt;/span&gt;&amp;#8217; to find an approximate match. &amp;#8216;&lt;span class="caps"&gt;TRUE&lt;/span&gt;(exact match)&amp;#8217; uses a linear search algorithm to step through the values looking for an exact match using a linear search algorithm. &amp;#8216;&lt;span class="caps"&gt;FALSE&lt;/span&gt;(approximate match)&amp;#8217; is perhaps somewhat misleading, as it doesnt match the closest textual match, but rather, returns the second highest value. It uses a binary search algoritms to continually search at the mid-point value. While this may be faster, it only works on sorted numerical data and if an exact match isnt found it returns the second largest&amp;nbsp;value.&lt;/p&gt;
&lt;h2&gt;Excel fuzzy&amp;nbsp;matching&lt;/h2&gt;
&lt;p&gt;This is not terribly useful. What we are looking for is something more powerful, which can help match text with slight spelling discrepencies or can search for matches acrosss multiple columns. The tool we need is called Fuzzy Matching, or more boardly speaking &lt;a href="https://en.wikipedia.org/wiki/Record_linkage#Probabilistic_record_linkage"&gt;Probabilistic record linkage&lt;/a&gt;. This works by assigning a a probability score that the text in one cell matches another, based on a chosen method. Recently  Microsoft released a tool to &lt;a href="https://support.microsoft.com/en-us/office/fuzzy-match-support-for-get-transform-power-query-ffdd5082-c0c8-4c8e-a794-bd3962b90649"&gt;merge tables using fuzzy matching&lt;/a&gt; (windows only). This allows you to set the following matching&amp;nbsp;options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Similarity Threshold – This option indicates how similar two values need to be in order to match. The minimum value of 0.00 will cause all values to match each other, and the maximum value of 1.00 will only allow exact matches. The default is&amp;nbsp;0.80.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ignore case – This option indicates whether text values should be compared in a case sensitive or insensitive setting. The default behavior is case insensitive, which ignores&amp;nbsp;casing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximum number of matches – This option controls the maximum number of matching rows that will be returned for each input row. For example, if you only want to find one matching row for each input row, specify a value of 1. The default behavior is to return all&amp;nbsp;matches.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformation table – This option allows users to specify another query that holds a mapping table, so that some values can be auto-mapped as part of the matching logic. For example, defining a two-column table with a “From” and “To” text columns with values “Microsoft” and “&lt;span class="caps"&gt;MSFT&lt;/span&gt;” will make these two values be considered the same (similarity score of 1.00) by the matching&amp;nbsp;logic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Record matching using&amp;nbsp;Python&lt;/h2&gt;
&lt;p&gt;If you want more options for your record linkages, you use macOS, or if you want more control over your workflow, Python is a good option, especially using the &lt;a href="https://github.com/RobinL/fuzzymatcher"&gt;fuzzymatcher&lt;/a&gt; or &lt;a href="https://github.com/J535D165/recordlinkage"&gt;Python Record Linkage Toolkit&lt;/a&gt;&amp;nbsp;libraries.&lt;/p&gt;
&lt;h4&gt;fuzzymatcher&amp;nbsp;examples&lt;/h4&gt;
&lt;p&gt;Here is a basic example from the fuzzymatcher docs linking two&amp;nbsp;tables.&lt;/p&gt;
&lt;p&gt;Table&amp;nbsp;A &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;ons_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Darlington&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Monmouthshire&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Havering&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Knowsley&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Charnwood&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;and Table&amp;nbsp;B&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;id&lt;/th&gt;
&lt;th&gt;os_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Darlington (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Havering London Boro&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Sir Fynwy - Monmouthshire&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Knowsley District (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Charnwood District (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Writing the&amp;nbsp;following&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;fuzzymatcher&lt;/span&gt;  
&lt;span class="n"&gt;fuzzymatcher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fuzzy_left_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left_on&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ons_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_on&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;os_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;gives&amp;nbsp;you:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;best_match_score&lt;/th&gt;
&lt;th&gt;ons_name&lt;/th&gt;
&lt;th&gt;os_name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.178449&lt;/td&gt;
&lt;td&gt;Darlington&lt;/td&gt;
&lt;td&gt;Darlington (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.133371&lt;/td&gt;
&lt;td&gt;Monmouthshire&lt;/td&gt;
&lt;td&gt;Sir Fynwy - Monmouthshire&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.102473&lt;/td&gt;
&lt;td&gt;Havering&lt;/td&gt;
&lt;td&gt;Havering London Boro&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.155775&lt;/td&gt;
&lt;td&gt;Knowsley&lt;/td&gt;
&lt;td&gt;Knowsley District (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.155775&lt;/td&gt;
&lt;td&gt;Charnwood&lt;/td&gt;
&lt;td&gt;Charnwood District (B)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="How-to"></category><category term="python"></category><category term="information management"></category></entry><entry><title>Analyzing a post lockdown hike</title><link href="https://brianmcdonald.me/analyzing-a-post-covid19-hike.html" rel="alternate"></link><published>2020-05-17T10:20:00+02:00</published><updated>2020-03-04T18:40:00+01:00</updated><author><name>Brian Mc Donald</name></author><id>tag:brianmcdonald.me,2020-05-17:/analyzing-a-post-covid19-hike.html</id><summary type="html">&lt;p&gt;The end of the this &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 lockdown was a chance to to go hiking in the mountains again and to examine some of the quirks of &lt;span class="caps"&gt;GPS&lt;/span&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Going without something for a long time can sharpen your sense of appreciation for it. Since France announced lockdown measures on 17th March, my experiences with the physical world outside of my apartment have been limited to trips to buy groceries and short walks around the outside of &lt;a href="http://www.chateau-ferney-voltaire.fr/en"&gt;Chateau Voltaire&lt;/a&gt;. Yesterday, as the restriction began to ease, I was finally able to go for a short hike in &lt;a href="https://www.openstreetmap.org/search?query=sommand#map=14/46.1612/6.5524"&gt;Sommand&lt;/a&gt; in &lt;a href="https://en.wikipedia.org/wiki/Haute-Savoie"&gt;Haute-Savoie&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The hike itself only took a couple of hours and was not difficult - although my lockdown conditioned muscles argue differently with me this morning. Below are a static 3D map and dynamic map of the hike along with some pictures of the views and local&amp;nbsp;wildlife.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chamois" src="files/images/chamois.jpg"&gt; &lt;br&gt;
&lt;em&gt;A Chamois (Credit: &lt;a href="https://en.wikipedia.org/wiki/Chamois"&gt;Wikipedia&lt;/a&gt; Manfred&amp;nbsp;Werner)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The&amp;nbsp;route&lt;/h2&gt;
&lt;p&gt;We parked in the small parking area past La Matafan, You can see it in the map below, the section where the loop doesnt quite join up.
&lt;img alt="Route" src="files/images/Sommand-hike.jpg"&gt; &lt;br&gt;
&lt;em&gt;3D render of the route using &lt;span class="caps"&gt;QGIS&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="/Map1.html"&gt;&lt;img alt="Vis" src="files/images/Sommand-trail.webp"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Click to see the full interactive&amp;nbsp;map&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You&amp;#8217;ll notice we took a long route on the way back to the car. That was to get ice-cream&amp;nbsp;:)&lt;/p&gt;
&lt;h2&gt;Elevation&amp;nbsp;profiles&lt;/h2&gt;
&lt;p&gt;The trails in the above maps are from our recorded route, logged in the wonderful &lt;a href="https://osmand.net/"&gt;OsmAnd&lt;/a&gt; Android app. The &lt;span class="caps"&gt;GPS&lt;/span&gt; in my basic android phone supposedly uses &lt;a href="https://en.wikipedia.org/wiki/Assisted_GPS"&gt;A-&lt;span class="caps"&gt;GPS&lt;/span&gt;&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/GLONASS"&gt;&lt;span class="caps"&gt;GLOSNASS&lt;/span&gt;&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/BeiDou"&gt;&lt;span class="caps"&gt;BDS&lt;/span&gt;&lt;/a&gt; (but interestingly not &lt;a href="https://en.wikipedia.org/wiki/Galileo_(satellite_navigation)"&gt;Galileo&lt;/a&gt;) to log coordinates, so should be reasonably&amp;nbsp;accurate.&lt;/p&gt;
&lt;p&gt;I was curious as to how the elevation readings of my phone compare to other methods, such as calculating it from position on a &lt;span class="caps"&gt;DEM&lt;/span&gt; (Digital Elevation Model) such as the &lt;span class="caps"&gt;SRTM&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Elevation" src="files/images/elev.webp"&gt;&lt;/p&gt;
&lt;p&gt;Comparing the two elevation sources shows some interesting results. While the tracks are quite well aligned for the section of trip where we descend on the eastern, south-eastern slope of the mountain, there is a big discrepancy in the section leading up to the highest point of our&amp;nbsp;hike.&lt;/p&gt;</content><category term="General"></category><category term="hiking"></category><category term="analysis"></category><category term="nature"></category></entry></feed>